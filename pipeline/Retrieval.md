## 3.2 Retrieval

Retrieval is implemented primarily via **LangChain retrievers**. We use a **hybrid retrieval** setup because **dense (vector) retrieval** and **BM25 (sparse) retrieval** are complementary:

- **Vector retrieval** focuses on *semantic similarity* and improves generalization beyond exact keyword overlap.
- **BM25 retrieval** focuses on *lexical matching* and is strong for exact keywords, entities, and terminology overlap.

Both are widely used in industry, so this project selects these two approaches as the core retrieval stack.

---

### 3.2.1 Dense (Vector) Retrieval

Dense retrieval uses **FAISS** for index construction and nearest-neighbor search, with embeddings generated by **M3E-large**.

**M3E** stands for **Moka Massive Mixed Embedding**:

- **Moka**: trained, open-sourced, and evaluated by **MokaAI**; training scripts use **uniem**, and evaluation uses the **MTEB-zh** benchmark.
- **Massive**: trained on a very large-scale Chinese paired dataset (â‰ˆ tens of millions / 22M+).
- **Mixed**: supports both Chinese and English semantic similarity, cross-type retrieval, and is designed to extend to code retrieval.
- **Embedding**: a text embedding model that maps natural language into dense vectors.

M3E provides multiple model sizes (**small**, **base**, **large**). This project uses **M3E-large**.

---

### 3.2.2 Sparse Retrieval (BM25)

**BM25** is a classic sparse retrieval algorithm used to score relevance between:
- a query and a document, or
- two texts (treated as query/document).

Core intuition:
- For each query term `q_i`, compute its contribution to relevance with respect to a candidate document.
- Aggregate all term contributions via a weighted sum to produce a final relevance score between the query and the document.
